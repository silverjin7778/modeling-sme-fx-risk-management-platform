# ----------------------------------------------------------
# # ë°ì´í„° ìˆ˜ì§‘
# ----------------------------------------------------------

import yfinance as yf
import pandas as pd

def yuan_indicator():
    tickers = {
    "Shanghai Composite": "000001.SS",
    "Shenzhen Component": "399001.SZ",
    "Hang Seng": "^HSI",
    "Hang Seng China Enterprises": "^HSCE"
    }
    
    data_list = []
    
    for name, symbol in tickers.items():
        ticker = yf.Ticker(symbol)
        df = ticker.history(period="25y", interval="1d")
        df = df.reset_index()
        
    
        df = df[["Date", "Open", "High", "Low", "Close"]]
        df = df.rename(columns={
            "Open": f"{name}_Open",
            "High": f"{name}_High",
            "Low": f"{name}_Low",
            "Close": f"{name}_Close"
        })
        df["Date"] = pd.to_datetime(df["Date"]).dt.tz_localize(None)
        data_list.append(df)
    
    df_merged = data_list[0]
    for df in data_list[1:]:
        df['Date']=pd.to_datetime(df['Date'])
        df_merged = pd.merge(df_merged, df, on="Date", how="inner")
    
    df_merged['Date'] = pd.to_datetime(df_merged['Date']).dt.strftime('%Y-%m-%d')
    df_merged['Date'] = pd.to_datetime(df_merged['Date'])
    return df_merged



import cloudpickle
with open("yuan_indicator.pkl", "wb") as f:
    cloudpickle.dump(yuan_indicator, f)


yuan_indi_df=yuan_indicator()



yuan_df=pd.read_csv('CNY_KRW ê³¼ê±° ë°ì´í„°.csv')


yuan_df['ë³€ë™ %']=yuan_df['ë³€ë™ %'].apply(lambda x: x.replace('%','')).astype(float)
yuan_df=yuan_df.drop('ê±°ë˜ëŸ‰',axis=1)
yuan_df=yuan_df.rename(columns={'ë‚ ì§œ':'Date','ì¢…ê°€':'Close','ì‹œê°€':'Open','ì €ê°€':'Low','ê³ ê°€':'High','ë³€ë™ %':'Change'})

import datetime as dt
yuan_df['Date']=pd.to_datetime(pd.to_datetime(yuan_df['Date'],format='%Y- %m- %d').dt.strftime('%Y-%m-%d'))


yuan_df=yuan_df.sort_values('Date',ascending=True,ignore_index=True)


def CNYKRW():
    from selenium import webdriver
    from selenium.webdriver.chrome.service import Service
    from selenium.webdriver.common.by import By
    from datetime import datetime
    import pandas as pd
    import time

    df = pd.DataFrame(columns=["Date", "Close", "Open", "High", "Low", "Change"])
    
    options = webdriver.ChromeOptions()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    service = Service()
    
    driver = webdriver.Chrome(service=service, options=options)
    url = "https://kr.investing.com/currencies/cny-krw-historical-data"
    driver.get(url)
    time.sleep(5)

    def clean_number(text):
        return float(text.replace(',', '').replace('%', ''))

    for row in range(1, 3):  # tr[1], tr[2]
        try:
            date_xpath = f'//*[@id="__next"]/div[2]/div[2]/div[2]/div[1]/div[2]/div[3]/table/tbody/tr[{row}]/td[1]'
            close_xpath = f'//*[@id="__next"]/div[2]/div[2]/div[2]/div[1]/div[2]/div[3]/table/tbody/tr[{row}]/td[2]'
            open_xpath = f'//*[@id="__next"]/div[2]/div[2]/div[2]/div[1]/div[2]/div[3]/table/tbody/tr[{row}]/td[3]'
            high_xpath = f'//*[@id="__next"]/div[2]/div[2]/div[2]/div[1]/div[2]/div[3]/table/tbody/tr[{row}]/td[4]'
            low_xpath = f'//*[@id="__next"]/div[2]/div[2]/div[2]/div[1]/div[2]/div[3]/table/tbody/tr[{row}]/td[5]'
            change_xpath = f'//*[@id="__next"]/div[2]/div[2]/div[2]/div[1]/div[2]/div[3]/table/tbody/tr[{row}]/td[7]'

            date = driver.find_element(By.XPATH, date_xpath).text
            close = driver.find_element(By.XPATH, close_xpath).text
            open_ = driver.find_element(By.XPATH, open_xpath).text
            high = driver.find_element(By.XPATH, high_xpath).text
            low = driver.find_element(By.XPATH, low_xpath).text
            change = driver.find_element(By.XPATH, change_xpath).text

            # ë‚ ì§œ ë³€í™˜
            date = datetime.strptime(date.replace('ì›”', '-'), '%m- %d, %Y').date()

            new_row = {
                "Date": pd.to_datetime(date).normalize(),
                "Close": clean_number(close),
                "Open": clean_number(open_),
                "High": clean_number(high),
                "Low": clean_number(low),
                "Change": clean_number(change)
            }

            # ì¤‘ë³µ í™•ì¸ í›„ ì¶”ê°€
            if new_row["Date"] not in df["Date"].values:
                df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)

        except Exception as e:
            print(f"[âŒ ERROR] Row {row}: {e}")
            continue

    df = df.sort_values("Date").reset_index(drop=True)
    driver.quit()
    return df

import cloudpickle
with open("yuan_real_times.pkl", "wb") as f:
    cloudpickle.dump(CNYKRW, f)


yuan_df=pd.concat([yuan_df,CNYKRW()],ignore_index=True).drop_duplicates(subset='Date',keep='last',ignore_index=True)
yuan_df["Change"] = yuan_df["Close"].diff()

yuan_df.to_csv('yuan_df.csv',index=False)

all_df=pd.merge(yuan_indi_df,yuan_df,on='Date',how='inner')

df_base=all_df





# ----------------------------------------------------------
# # í•˜ë£¨ ì˜ˆì¸¡
# ----------------------------------------------------------



# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from xgboost import XGBRegressor
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.callbacks import EarlyStopping

# 2. ë°ì´í„° ì¤€ë¹„
df = df_base.copy()  # ê¸°ì¡´ df_baseë¥¼ ìœ ì§€
df = df.sort_values('Date')
df['Date'] = pd.to_datetime(df['Date'])
df['target'] = df['Close'].shift(-1)
df = df.dropna().reset_index(drop=True)

# 3. í”¼ì²˜, íƒ€ê²Ÿ ì„¤ì •
drop_cols = ['Date', 'ê¸°ì¤€ë…„ì›”', 'return', 'return_future', 'target', 'next_day_close']
X = df.drop(columns=drop_cols, errors='ignore')
y = df['target'].values

# 4. í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• 
split_idx = int(len(df) * 0.8)
X_train_raw, X_test_raw = X.iloc[:split_idx], X.iloc[split_idx:]
y_train_raw, y_test_raw = y[:split_idx], y[split_idx:]

# 5. ì •ê·œí™”
scaler_X = MinMaxScaler().fit(X_train_raw)
X_train_scaled = scaler_X.transform(X_train_raw)
X_test_scaled = scaler_X.transform(X_test_raw)

scaler_y = MinMaxScaler().fit(y_train_raw.reshape(-1, 1))
y_train_scaled = scaler_y.transform(y_train_raw.reshape(-1, 1))
y_test_scaled = scaler_y.transform(y_test_raw.reshape(-1, 1))

# 6. ì‹œí€€ìŠ¤ ìƒì„± í•¨ìˆ˜
def make_sequence(X, y, seq_len):
    X_seq, y_seq = [], []
    for i in range(len(X) - seq_len):
        X_seq.append(X[i:i+seq_len])
        y_seq.append(y[i+seq_len])
    return np.array(X_seq), np.array(y_seq)

seq_len = 20
X_train_seq, y_train_seq = make_sequence(X_train_scaled, y_train_scaled, seq_len)
X_test_seq, y_test_seq = make_sequence(X_test_scaled, y_test_scaled, seq_len)

X_train_flat = X_train_scaled[seq_len:][:len(X_train_seq)]
X_test_flat  = X_test_scaled[seq_len:][:len(X_test_seq)]
y_train_flat = y_train_scaled[seq_len:][:len(X_train_seq)]
y_test_flat  = y_test_scaled[seq_len:][:len(X_test_seq)]

# 7. ëª¨ë¸ í›ˆë ¨ ë° ì˜ˆì¸¡
## XGBoost
model_xgb = XGBRegressor(n_estimators=200, learning_rate=0.05, random_state=42)
model_xgb.fit(X_train_flat, y_train_flat.ravel())
y_pred_xgb = scaler_y.inverse_transform(model_xgb.predict(X_test_flat).reshape(-1, 1)).reshape(-1)

## LSTM
model_lstm = Sequential()
model_lstm.add(LSTM(128, input_shape=(seq_len, X_train_seq.shape[2])))
model_lstm.add(Dense(1))
model_lstm.compile(optimizer='adam', loss='mse')
early_stop = EarlyStopping(patience=15, restore_best_weights=True)
model_lstm.fit(X_train_seq, y_train_seq, epochs=100, batch_size=32, validation_split=0.2,
               callbacks=[early_stop], verbose=0)
y_pred_lstm = scaler_y.inverse_transform(model_lstm.predict(X_test_seq)).reshape(-1)

## RandomForest
model_rf = RandomForestRegressor(n_estimators=200, random_state=42)
model_rf.fit(X_train_flat, y_train_flat.ravel())
y_pred_rf = scaler_y.inverse_transform(model_rf.predict(X_test_flat).reshape(-1, 1)).reshape(-1)

# 8. ì‹¤ì œê°’ ë³µì›
y_true = scaler_y.inverse_transform(y_test_seq).reshape(-1)

# 9. ì„±ëŠ¥ ì¶œë ¥
for name, pred in zip(['XGBoost', 'LSTM', 'RandomForest'],
                      [y_pred_xgb, y_pred_lstm, y_pred_rf]):
    rmse = np.sqrt(mean_squared_error(y_true, pred))
    mae = mean_absolute_error(y_true, pred)
    r2 = r2_score(y_true, pred)
    print(f"ğŸ“Š {name}")
    print(f"    MAE:  {mae:.4f}")
    print(f"    RMSE: {rmse:.4f}")
    print(f"    RÂ²:   {r2:.4f}")

# 10. ì‹œê°í™”
date_test = df['Date'].iloc[seq_len + split_idx:].reset_index(drop=True)
n = min(len(date_test), len(y_true), len(y_pred_xgb), len(y_pred_lstm), len(y_pred_rf))

plt.figure(figsize=(14, 6))
plt.plot(date_test[:n], y_true[:n], label='ì‹¤ì œ ì¢…ê°€', linewidth=2)
plt.plot(date_test[:n], y_pred_xgb[:n], label='XGBoost ì˜ˆì¸¡', linestyle='--')
plt.plot(date_test[:n], y_pred_lstm[:n], label='LSTM ì˜ˆì¸¡', linestyle='--')
plt.plot(date_test[:n], y_pred_rf[:n], label='RandomForest ì˜ˆì¸¡', linestyle='--')
plt.title("ëª¨ë¸ë³„ í•˜ë£¨ ë’¤ ì¢…ê°€ ì˜ˆì¸¡ ë¹„êµ")
plt.xlabel("ë‚ ì§œ")
plt.ylabel("Close")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()


# In[126]:


import joblib
joblib.dump(model_lstm, 'yuan_í•˜ë£¨.pkl')
joblib.dump(scaler_X, 'yuan_scaler_X_í•˜ë£¨.pkl')
joblib.dump(scaler_y, 'yuan_scaler_y_í•˜ë£¨.pkl')





# ----------------------------------------------------------
# # ì¼ì£¼ì¼ ì˜ˆì¸¡
# ----------------------------------------------------------


# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from xgboost import XGBRegressor
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.callbacks import EarlyStopping

# 2. ë°ì´í„° ì¤€ë¹„
df = df_base.copy()  # ê¸°ì¡´ df_baseë¥¼ ìœ ì§€
df = df.sort_values('Date')
df['Date'] = pd.to_datetime(df['Date'])
df['target'] = df['Close'].shift(-5)
df = df.dropna().reset_index(drop=True)

# 3. í”¼ì²˜, íƒ€ê²Ÿ ì„¤ì •
drop_cols = ['Date', 'ê¸°ì¤€ë…„ì›”', 'return', 'return_future', 'target', 'next_day_close']
X = df.drop(columns=drop_cols, errors='ignore')
y = df['target'].values

# 4. í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• 
split_idx = int(len(df) * 0.8)
X_train_raw, X_test_raw = X.iloc[:split_idx], X.iloc[split_idx:]
y_train_raw, y_test_raw = y[:split_idx], y[split_idx:]

# 5. ì •ê·œí™”
scaler_X = MinMaxScaler().fit(X_train_raw)
X_train_scaled = scaler_X.transform(X_train_raw)
X_test_scaled = scaler_X.transform(X_test_raw)

scaler_y = MinMaxScaler().fit(y_train_raw.reshape(-1, 1))
y_train_scaled = scaler_y.transform(y_train_raw.reshape(-1, 1))
y_test_scaled = scaler_y.transform(y_test_raw.reshape(-1, 1))

# 6. ì‹œí€€ìŠ¤ ìƒì„± í•¨ìˆ˜
def make_sequence(X, y, seq_len):
    X_seq, y_seq = [], []
    for i in range(len(X) - seq_len):
        X_seq.append(X[i:i+seq_len])
        y_seq.append(y[i+seq_len])
    return np.array(X_seq), np.array(y_seq)

seq_len = 20
X_train_seq, y_train_seq = make_sequence(X_train_scaled, y_train_scaled, seq_len)
X_test_seq, y_test_seq = make_sequence(X_test_scaled, y_test_scaled, seq_len)

X_train_flat = X_train_scaled[seq_len:][:len(X_train_seq)]
X_test_flat  = X_test_scaled[seq_len:][:len(X_test_seq)]
y_train_flat = y_train_scaled[seq_len:][:len(X_train_seq)]
y_test_flat  = y_test_scaled[seq_len:][:len(X_test_seq)]

# 7. ëª¨ë¸ í›ˆë ¨ ë° ì˜ˆì¸¡
## XGBoost
model_xgb = XGBRegressor(n_estimators=200, learning_rate=0.05, random_state=42)
model_xgb.fit(X_train_flat, y_train_flat.ravel())
y_pred_xgb = scaler_y.inverse_transform(model_xgb.predict(X_test_flat).reshape(-1, 1)).reshape(-1)

## LSTM
model_lstm = Sequential()
model_lstm.add(LSTM(128, input_shape=(seq_len, X_train_seq.shape[2])))
model_lstm.add(Dense(1))
model_lstm.compile(optimizer='adam', loss='mse')
early_stop = EarlyStopping(patience=15, restore_best_weights=True)
model_lstm.fit(X_train_seq, y_train_seq, epochs=100, batch_size=32, validation_split=0.2,
               callbacks=[early_stop], verbose=0)
y_pred_lstm = scaler_y.inverse_transform(model_lstm.predict(X_test_seq)).reshape(-1)

## RandomForest
model_rf = RandomForestRegressor(n_estimators=200, random_state=42)
model_rf.fit(X_train_flat, y_train_flat.ravel())
y_pred_rf = scaler_y.inverse_transform(model_rf.predict(X_test_flat).reshape(-1, 1)).reshape(-1)

# 8. ì‹¤ì œê°’ ë³µì›
y_true = scaler_y.inverse_transform(y_test_seq).reshape(-1)

# 9. ì„±ëŠ¥ ì¶œë ¥
for name, pred in zip(['XGBoost', 'LSTM', 'RandomForest'],
                      [y_pred_xgb, y_pred_lstm, y_pred_rf]):
    rmse = np.sqrt(mean_squared_error(y_true, pred))
    mae = mean_absolute_error(y_true, pred)
    r2 = r2_score(y_true, pred)
    print(f"ğŸ“Š {name}")
    print(f"    MAE:  {mae:.4f}")
    print(f"    RMSE: {rmse:.4f}")
    print(f"    RÂ²:   {r2:.4f}")

# 10. ì‹œê°í™”
date_test = df['Date'].iloc[seq_len + split_idx:].reset_index(drop=True)
n = min(len(date_test), len(y_true), len(y_pred_xgb), len(y_pred_lstm), len(y_pred_rf))

plt.figure(figsize=(14, 6))
plt.plot(date_test[:n], y_true[:n], label='ì‹¤ì œ ì¢…ê°€', linewidth=2)
plt.plot(date_test[:n], y_pred_xgb[:n], label='XGBoost ì˜ˆì¸¡', linestyle='--')
plt.plot(date_test[:n], y_pred_lstm[:n], label='LSTM ì˜ˆì¸¡', linestyle='--')
plt.plot(date_test[:n], y_pred_rf[:n], label='RandomForest ì˜ˆì¸¡', linestyle='--')
plt.title("ëª¨ë¸ë³„ ì¼ì£¼ì¼ ë’¤ ì¢…ê°€ ì˜ˆì¸¡ ë¹„êµ")
plt.xlabel("ë‚ ì§œ")
plt.ylabel("Close")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()


# In[128]:


import joblib
joblib.dump(model_lstm, 'yuan_ì¼ì£¼ì¼.pkl')
joblib.dump(scaler_X, 'yuan_scaler_X_ì¼ì£¼ì¼.pkl')
joblib.dump(scaler_y, 'yuan_scaler_y_ì¼ì£¼ì¼.pkl')






# ----------------------------------------------------------
# # í•œë‹¬ ì˜ˆì¸¡
# ----------------------------------------------------------


# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from xgboost import XGBRegressor
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.callbacks import EarlyStopping

# 2. ë°ì´í„° ì¤€ë¹„
df = df_base.copy()  # ê¸°ì¡´ df_baseë¥¼ ìœ ì§€
df = df.sort_values('Date')
df['Date'] = pd.to_datetime(df['Date'])
df['target'] = df['Close'].shift(-20)
df = df.dropna().reset_index(drop=True)

# 3. í”¼ì²˜, íƒ€ê²Ÿ ì„¤ì •
drop_cols = ['Date', 'ê¸°ì¤€ë…„ì›”', 'return', 'return_future', 'target', 'next_day_close']
X = df.drop(columns=drop_cols, errors='ignore')
y = df['target'].values

# 4. í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• 
split_idx = int(len(df) * 0.8)
X_train_raw, X_test_raw = X.iloc[:split_idx], X.iloc[split_idx:]
y_train_raw, y_test_raw = y[:split_idx], y[split_idx:]

# 5. ì •ê·œí™”
scaler_X = MinMaxScaler().fit(X_train_raw)
X_train_scaled = scaler_X.transform(X_train_raw)
X_test_scaled = scaler_X.transform(X_test_raw)

scaler_y = MinMaxScaler().fit(y_train_raw.reshape(-1, 1))
y_train_scaled = scaler_y.transform(y_train_raw.reshape(-1, 1))
y_test_scaled = scaler_y.transform(y_test_raw.reshape(-1, 1))

# 6. ì‹œí€€ìŠ¤ ìƒì„± í•¨ìˆ˜
def make_sequence(X, y, seq_len):
    X_seq, y_seq = [], []
    for i in range(len(X) - seq_len):
        X_seq.append(X[i:i+seq_len])
        y_seq.append(y[i+seq_len])
    return np.array(X_seq), np.array(y_seq)

seq_len = 20
X_train_seq, y_train_seq = make_sequence(X_train_scaled, y_train_scaled, seq_len)
X_test_seq, y_test_seq = make_sequence(X_test_scaled, y_test_scaled, seq_len)

X_train_flat = X_train_scaled[seq_len:][:len(X_train_seq)]
X_test_flat  = X_test_scaled[seq_len:][:len(X_test_seq)]
y_train_flat = y_train_scaled[seq_len:][:len(X_train_seq)]
y_test_flat  = y_test_scaled[seq_len:][:len(X_test_seq)]

# 7. ëª¨ë¸ í›ˆë ¨ ë° ì˜ˆì¸¡
## XGBoost
model_xgb = XGBRegressor(n_estimators=200, learning_rate=0.05, random_state=42)
model_xgb.fit(X_train_flat, y_train_flat.ravel())
y_pred_xgb = scaler_y.inverse_transform(model_xgb.predict(X_test_flat).reshape(-1, 1)).reshape(-1)

## LSTM
model_lstm = Sequential()
model_lstm.add(LSTM(128, input_shape=(seq_len, X_train_seq.shape[2])))
model_lstm.add(Dense(1))
model_lstm.compile(optimizer='adam', loss='mse')
early_stop = EarlyStopping(patience=15, restore_best_weights=True)
model_lstm.fit(X_train_seq, y_train_seq, epochs=100, batch_size=32, validation_split=0.2,
               callbacks=[early_stop], verbose=0)
y_pred_lstm = scaler_y.inverse_transform(model_lstm.predict(X_test_seq)).reshape(-1)

## RandomForest
model_rf = RandomForestRegressor(n_estimators=200, random_state=42)
model_rf.fit(X_train_flat, y_train_flat.ravel())
y_pred_rf = scaler_y.inverse_transform(model_rf.predict(X_test_flat).reshape(-1, 1)).reshape(-1)

# 8. ì‹¤ì œê°’ ë³µì›
y_true = scaler_y.inverse_transform(y_test_seq).reshape(-1)

# 9. ì„±ëŠ¥ ì¶œë ¥
for name, pred in zip(['XGBoost', 'LSTM', 'RandomForest'],
                      [y_pred_xgb, y_pred_lstm, y_pred_rf]):
    rmse = np.sqrt(mean_squared_error(y_true, pred))
    mae = mean_absolute_error(y_true, pred)
    r2 = r2_score(y_true, pred)
    print(f"ğŸ“Š {name}")
    print(f"    MAE:  {mae:.4f}")
    print(f"    RMSE: {rmse:.4f}")
    print(f"    RÂ²:   {r2:.4f}")

# 10. ì‹œê°í™”
date_test = df['Date'].iloc[seq_len + split_idx:].reset_index(drop=True)
n = min(len(date_test), len(y_true), len(y_pred_xgb), len(y_pred_lstm), len(y_pred_rf))

plt.figure(figsize=(14, 6))
plt.plot(date_test[:n], y_true[:n], label='ì‹¤ì œ ì¢…ê°€', linewidth=2)
plt.plot(date_test[:n], y_pred_xgb[:n], label='XGBoost ì˜ˆì¸¡', linestyle='--')
plt.plot(date_test[:n], y_pred_lstm[:n], label='LSTM ì˜ˆì¸¡', linestyle='--')
plt.plot(date_test[:n], y_pred_rf[:n], label='RandomForest ì˜ˆì¸¡', linestyle='--')
plt.title("ëª¨ë¸ë³„ í•œë‹¬ ë’¤ ì¢…ê°€ ì˜ˆì¸¡ ë¹„êµ")
plt.xlabel("ë‚ ì§œ")
plt.ylabel("Close")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()


# In[130]:


import joblib
joblib.dump(model_rf, 'yuan_í•œë‹¬.pkl')
joblib.dump(scaler_X, 'yuan_scaler_X_í•œë‹¬.pkl')
joblib.dump(scaler_y, 'yuan_scaler_y_í•œë‹¬.pkl')





# ----------------------------------------------------------
# # ì„¸ë‹¬ ì˜ˆì¸¡
# ----------------------------------------------------------


# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from xgboost import XGBRegressor
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.callbacks import EarlyStopping

# 2. ë°ì´í„° ì¤€ë¹„
df = df_base.copy()  # ê¸°ì¡´ df_baseë¥¼ ìœ ì§€
df = df.sort_values('Date')
df['Date'] = pd.to_datetime(df['Date'])
df['target'] = df['Close'].shift(-60)
df = df.dropna().reset_index(drop=True)

# 3. í”¼ì²˜, íƒ€ê²Ÿ ì„¤ì •
drop_cols = ['Date', 'ê¸°ì¤€ë…„ì›”', 'return', 'return_future', 'target', 'next_day_close']
X = df.drop(columns=drop_cols, errors='ignore')
y = df['target'].values

# 4. í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• 
split_idx = int(len(df) * 0.8)
X_train_raw, X_test_raw = X.iloc[:split_idx], X.iloc[split_idx:]
y_train_raw, y_test_raw = y[:split_idx], y[split_idx:]

# 5. ì •ê·œí™”
scaler_X = MinMaxScaler().fit(X_train_raw)
X_train_scaled = scaler_X.transform(X_train_raw)
X_test_scaled = scaler_X.transform(X_test_raw)

scaler_y = MinMaxScaler().fit(y_train_raw.reshape(-1, 1))
y_train_scaled = scaler_y.transform(y_train_raw.reshape(-1, 1))
y_test_scaled = scaler_y.transform(y_test_raw.reshape(-1, 1))

# 6. ì‹œí€€ìŠ¤ ìƒì„± í•¨ìˆ˜
def make_sequence(X, y, seq_len):
    X_seq, y_seq = [], []
    for i in range(len(X) - seq_len):
        X_seq.append(X[i:i+seq_len])
        y_seq.append(y[i+seq_len])
    return np.array(X_seq), np.array(y_seq)

seq_len = 20
X_train_seq, y_train_seq = make_sequence(X_train_scaled, y_train_scaled, seq_len)
X_test_seq, y_test_seq = make_sequence(X_test_scaled, y_test_scaled, seq_len)

X_train_flat = X_train_scaled[seq_len:][:len(X_train_seq)]
X_test_flat  = X_test_scaled[seq_len:][:len(X_test_seq)]
y_train_flat = y_train_scaled[seq_len:][:len(X_train_seq)]
y_test_flat  = y_test_scaled[seq_len:][:len(X_test_seq)]

# 7. ëª¨ë¸ í›ˆë ¨ ë° ì˜ˆì¸¡
## XGBoost
model_xgb = XGBRegressor(n_estimators=200, learning_rate=0.05, random_state=42)
model_xgb.fit(X_train_flat, y_train_flat.ravel())
y_pred_xgb = scaler_y.inverse_transform(model_xgb.predict(X_test_flat).reshape(-1, 1)).reshape(-1)

## LSTM
model_lstm = Sequential()
model_lstm.add(LSTM(128, input_shape=(seq_len, X_train_seq.shape[2])))
model_lstm.add(Dense(1))
model_lstm.compile(optimizer='adam', loss='mse')
early_stop = EarlyStopping(patience=15, restore_best_weights=True)
model_lstm.fit(X_train_seq, y_train_seq, epochs=100, batch_size=32, validation_split=0.2,
               callbacks=[early_stop], verbose=0)
y_pred_lstm = scaler_y.inverse_transform(model_lstm.predict(X_test_seq)).reshape(-1)

## RandomForest
model_rf = RandomForestRegressor(n_estimators=200, random_state=42)
model_rf.fit(X_train_flat, y_train_flat.ravel())
y_pred_rf = scaler_y.inverse_transform(model_rf.predict(X_test_flat).reshape(-1, 1)).reshape(-1)

# 8. ì‹¤ì œê°’ ë³µì›
y_true = scaler_y.inverse_transform(y_test_seq).reshape(-1)

# 9. ì„±ëŠ¥ ì¶œë ¥
for name, pred in zip(['XGBoost', 'LSTM', 'RandomForest'],
                      [y_pred_xgb, y_pred_lstm, y_pred_rf]):
    rmse = np.sqrt(mean_squared_error(y_true, pred))
    mae = mean_absolute_error(y_true, pred)
    r2 = r2_score(y_true, pred)
    print(f"ğŸ“Š {name}")
    print(f"    MAE:  {mae:.4f}")
    print(f"    RMSE: {rmse:.4f}")
    print(f"    RÂ²:   {r2:.4f}")

# 10. ì‹œê°í™”
date_test = df['Date'].iloc[seq_len + split_idx:].reset_index(drop=True)
n = min(len(date_test), len(y_true), len(y_pred_xgb), len(y_pred_lstm), len(y_pred_rf))

plt.figure(figsize=(14, 6))
plt.plot(date_test[:n], y_true[:n], label='ì‹¤ì œ ì¢…ê°€', linewidth=2)
plt.plot(date_test[:n], y_pred_xgb[:n], label='XGBoost ì˜ˆì¸¡', linestyle='--')
plt.plot(date_test[:n], y_pred_lstm[:n], label='LSTM ì˜ˆì¸¡', linestyle='--')
plt.plot(date_test[:n], y_pred_rf[:n], label='RandomForest ì˜ˆì¸¡', linestyle='--')
plt.title("ëª¨ë¸ë³„ ì„¸ë‹¬ ë’¤ ì¢…ê°€ ì˜ˆì¸¡ ë¹„êµ")
plt.xlabel("ë‚ ì§œ")
plt.ylabel("Close")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()


import joblib
joblib.dump(model_rf, 'yuan_ì„¸ë‹¬.pkl')
joblib.dump(scaler_X, 'yuan_scaler_X_ì„¸ë‹¬.pkl')
joblib.dump(scaler_y, 'yuan_scaler_y_ì„¸ë‹¬.pkl')
